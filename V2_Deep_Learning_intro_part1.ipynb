{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naomifridman/Introduction_to_deep_learning/blob/master/V2_Deep_Learning_intro_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSvdzEbqFD3Z"
      },
      "source": [
        "# Introduction to Deep Learning - Part 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cGmKs7z5HMIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "YAGlLRUCFD3e",
        "outputId": "db51435f-70a6-4e41-afb1-570c46225c16"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cc24781f8d91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mutils_plot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0muplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgridspec\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgridspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils_plot'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import utils_plot as uplot\n",
        "import matplotlib.gridspec as gridspec\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "%matplotlib inline  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHJr0EwfFD3g"
      },
      "source": [
        "## Perceptron\n",
        "Perceptron is a basic artificial neural network.<br>\n",
        "It contains:\n",
        "* a single layer with \n",
        "* linear activation function\n",
        "Perceptronis a linear classifier.<br>\n",
        "![title](perceptron_paradigma3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuZMMAmRFD3h"
      },
      "source": [
        "## Decision Units - Activation functions\n",
        "There are manyoptions for decision units, and we will see them later. Lets start with the basic one, defined by McCulloch-Pitts (1943)\n",
        "### Binary threshold unit\n",
        "* Compute weighted sum plus bias of the input features: \n",
        "$$z=b+\\sum_{i} x_{i}w_{i}$$\n",
        "* Then output 1 if weighted sum larger then zero.\n",
        "$$y = \\begin{cases} 1 & z \\geqslant  0 \\\\\n",
        "                       0 & otherwise \\\\\n",
        "                     \\end{cases}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQKsE7ahFD3h"
      },
      "outputs": [],
      "source": [
        "uplot.drow_binary_threshold()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV2b47HMFD3j"
      },
      "source": [
        "## Learning the Bias\n",
        "Now we need to learn the weights and the bias. But we can use a trick, toavoid separate scheme, for learning the bias. \n",
        "* We can add 1 to all input vectors: <br>\n",
        "For X = $(X_{0}, X_{1}, ...Xn)$ Lets define:  $\\bar{X}$ = $(X_{0}, X_{1}, ...Xn)$<br>\n",
        "=> $$z=b+\\sum_{i} x_{i}w_{i} = \\sum_{i} \\bar{x}_{i}\\bar{w}_{i}$$\n",
        "> This way, we can learn the bias as it was part of the weights. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjkA2hYhFD3j"
      },
      "source": [
        "## Learning weights\n",
        "The idea is to go over the samples, and correct/update the weights according to the results on the samples.\n",
        "![title](dl3.jpg)\n",
        "#### Update the weights:\n",
        "* Go over all training casesin anyorder, and:\n",
        "    * If the output is correct, continue\n",
        "    * If output is 1 and lable is 0 (\"increas W\"): $\\bar{W} = \\bar{W} + \\bar{X}_{i}$\n",
        "    * If output is 0 and lable is 1 (\"decreas W\"): $\\bar{W} = \\bar{W} - \\bar{X}_{i}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzRCKVN2FD3k"
      },
      "source": [
        "### Precpetron example - Logic OR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb8XP5YuFD3l"
      },
      "outputs": [],
      "source": [
        "def perceptron_plot(X, Y):\n",
        "    '''\n",
        "    train perceptron and plot the total loss in each epoch.\n",
        "    \n",
        "    :param X: data samples\n",
        "    :param Y: data labels\n",
        "    :return: weight vector as a numpy array\n",
        "    '''\n",
        "    w = np.zeros(len(X[0]))\n",
        "    eta = 1\n",
        "    n = 5\n",
        "    errors = []\n",
        "\n",
        "    for t in range(n):\n",
        "        \n",
        "        print('epoch: ' , t)\n",
        "        total_error = 0\n",
        "        \n",
        "        for i, x in enumerate(X):\n",
        "            \n",
        "            \n",
        "            if (np.dot(X[i], w) == Y[i]):\n",
        "                print(i, 'w*x==0', 'x',X[i],'w', w,'dot', np.dot(X[i], w), 'y',Y[i])\n",
        "                print('correct')\n",
        "                continue\n",
        "                \n",
        "            elif (np.dot(X[i], w) == 0 and Y[i] == 1):\n",
        "                total_error += 1\n",
        "                print(i, 'w*x==0 & y==1', 'X',X[i],'w', w,'dot',\n",
        "                      np.dot(X[i], w), 'y',Y[i])\n",
        "        \n",
        "                \n",
        "                w = w + X[i]\n",
        "                print('W updated to: ', w)\n",
        "                \n",
        "            elif (np.dot(X[i], w) == 1 and Y[i] == 0):\n",
        "                total_error += 1\n",
        "                print(i, 'w*x==1 & y==0' ,'X',X[i],'w', w,'dot', \n",
        "                      np.dot(X[i], w), 'y',Y[i], )\n",
        "                \n",
        "                w = w - X[i]\n",
        "                print('W updated to: ', w)\n",
        "                \n",
        "        errors.append(total_error)\n",
        "        \n",
        "    plt.plot(errors)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Total Loss')\n",
        "    \n",
        "    return w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAI8VJovFD3m"
      },
      "outputs": [],
      "source": [
        "X = [[1,0,0],\n",
        "    [1,1,0],\n",
        "    [1,0,1],\n",
        "    [1,1,1]]\n",
        "y = [0, 1, 1, 1]\n",
        "w=perceptron_plot(np.array(X),y)\n",
        "print('Leared Weights:', w)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHrLpa45FD3n"
      },
      "source": [
        "## OK this was learning, but were is the Deep ?\n",
        "* Well we can add more layers to our perceptron, \n",
        "* but it will not change nothing. <br>\n",
        "> Simple algebra show that any feed forward neural network, with linear activation function, can be reduced to a onelayer perceptron.\n",
        "\n",
        "* **A percepron with linear activation function, is a linear classifier.**<br>\n",
        "* **Linear Perceptron can distinguish data that is linearly separable.** <br>\n",
        "* **To exand learning, we need to use non-linear activation functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doF17fDoFD3n"
      },
      "source": [
        "## Multilayer perceptron - MLP\n",
        "A multilayer perceptron (MLP) is a class of feed-forward artificial neural network.<br>\n",
        "In MLPs some neurons use a nonlinear activation function that was developed with inspiration of the firing of biological neurons.<br>\n",
        "<br>\n",
        "* An MLP can containe many layers. \n",
        "* MLP node is a neuron that can use a nonlinear activation function. <br>\n",
        "* MLP utilizes a supervised learning technique called backpropagation for training - will be discussed inthe following chapters.\n",
        "> Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWwqW-NuFD3o"
      },
      "source": [
        "### Activation function\n",
        "#### Linear activation function\n",
        "If a multilayer perceptron has a linear activation function in all neurons, that is, a linear function that maps the weighted inputs to the output of each neuron, then linear algebra shows that any number of layers can be reduced to a two-layer input-output model. <br>\n",
        "<br>\n",
        "\n",
        "#### Non Linear Activation function\n",
        "* Allow model to generalize better\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfLmmFDxFD3o"
      },
      "source": [
        "#### Sigmoid activation function\n",
        "logistic function, which ranges from 0 to 1\n",
        "Used as last layer, garanty to output propability<br>\n",
        "The logistic sigmoid function can cause a neural network to get stuck at the training time.\n",
        "#### Logistic Sigmoid:  \n",
        "## $\\sigma(x)=\\frac{1}{1+e^{-x}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d64YU0iFFD3o"
      },
      "outputs": [],
      "source": [
        "import utils_plot as uplot\n",
        "uplot.drow_sigmoid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbkpBLv0FD3p"
      },
      "source": [
        "#### Tanh Sigmoid:  \n",
        "hyperbolic tangent that ranges from -1 to 1\n",
        "## $\\tanh(x)=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-triUmWDFD3p"
      },
      "outputs": [],
      "source": [
        "uplot.drow_tanh()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr-Ib-zKFD3p"
      },
      "source": [
        "### ReLU (Rectified Linear Unit) Activation Function\n",
        "\n",
        "The ReLU is the most used activation function in the world right now.Since, it is used in almost all the convolutional neural networks or deep learning.<br>\n",
        "## $relu(x)=max(0,x)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SO0KEtAtFD3p"
      },
      "outputs": [],
      "source": [
        "relu = lambda x:  np.maximum(0, x)\n",
        "uplot.drow_function(func=relu, func_name='Rectified Linear Unit',\n",
        "                 func_formula = r'$relu(x)=max(0,x)$')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45KynKh2FD3q"
      },
      "source": [
        "## Handwritten Digit Classification (Minst) \n",
        "### Classification example\n",
        "Based on:\n",
        "* https://www.coursera.org/learn/neural-networks/lecture/zO1Is/a-simple-example-of-learning-6-min\n",
        "We will train a simple Linear FFN -Perceptron on Minst data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIZ1cfLNFD3q"
      },
      "source": [
        "#### Load Minst Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-48SUIFFD3q"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 20\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, y_train = shuffle(x_train, y_train , random_state=0)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255.\n",
        "x_test /= 255.\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAZooficFD3q"
      },
      "source": [
        "#### Lest View some examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeWB-nXnFD3r"
      },
      "outputs": [],
      "source": [
        "print(x_train[0].shape)\n",
        "print('Lables are: ', y_train[0:8])\n",
        "uplot.show_n_images(x_train[0:8])\n",
        "print('Lables are: ', y_train[80:88])\n",
        "uplot.show_n_images(x_train[80:88])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOP5kn1vFD3r"
      },
      "source": [
        "### MLP Scheme\n",
        "* **Weights** will have following shape:  (n_classes, 28*28+1) \n",
        "> The one is for the bias, For clarity, we will drow only the first 28*28 weights\n",
        "\n",
        "* **Predict** - predict the class that has the \"larger impact\" in W[i]*X\n",
        "> We canmesaure \"larger imapct by sum or mean of the multiplication.\n",
        "\n",
        "![title](d6.png)\n",
        "#### Update the weights\n",
        "* **Correct** - If predictionis correct, do nothing\n",
        "* **Misclasified** - if prediction is wrong:\n",
        "    * Increas the weights of the correct class by input's active pixels \n",
        "    * Decrease the weights of misclassified class by input's active pixels \n",
        "<br>\n",
        "![title](d5.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjHIC_FzFD3r"
      },
      "outputs": [],
      "source": [
        "def predict(w, x):\n",
        "\n",
        "    pred_val = np.dot(w, np.append(x, 1))\n",
        "    pred_class = np.argmax(pred_val)\n",
        "    return pred_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymjc54UiFD3r"
      },
      "outputs": [],
      "source": [
        "def clasify_minst(X, Y,n_classes=10,num=100, epoch=1):\n",
        "    \n",
        "       \n",
        "    np.random.seed(7)\n",
        "\n",
        "    #w = np.zeros((n_classes,28*28+1))\n",
        "    w = np.random.random_sample((n_classes,28*28+1))\n",
        "    \n",
        "    n_epoch = epoch\n",
        "    num = num\n",
        "\n",
        "    for j in range(n_epoch):\n",
        "        \n",
        "        acc = 0\n",
        "        order = np.random.permutation(num)\n",
        "        \n",
        "        for i in range(num):\n",
        "\n",
        "            x, y = X[order[i]], Y[order[i]]\n",
        "            yhat = predict(w, x)\n",
        "            \n",
        "            if y == yhat:\n",
        "                acc += 1\n",
        "            else:\n",
        "                #w_update(x, y, yhat)\n",
        "                w[y] += np.append(x, 1)\n",
        "                w[yhat] -= np.append(x, 1)\n",
        "            \n",
        "        accuracy = acc / float(num)\n",
        "        print ('Iteration %d: acc = %f' % (j+1, accuracy))\n",
        "                \n",
        "    return w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvoC9lz-FD3r"
      },
      "outputs": [],
      "source": [
        "w = clasify_minst(x_train, y_train,num=1,epoch=1)\n",
        "uplot.show_n_images([w[i,0:28*28].reshape((28,28)) for i in range(10)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwAJZrYVFD3s"
      },
      "outputs": [],
      "source": [
        "plt.imshow(x_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zD2yEMMaFD3s"
      },
      "outputs": [],
      "source": [
        "w = clasify_minst(x_train, y_train, num=100, epoch=1)\n",
        "uplot.show_n_images([w[i,0:28*28].reshape((28,28)) for i in range(10)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9fcD5LuFD3t"
      },
      "outputs": [],
      "source": [
        "w = clasify_minst(x_train, y_train, num=500, epoch=5)\n",
        "uplot.show_n_images([w[i,0:28*28].reshape((28,28)) for i in range(10)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqcD7zSyFD3t"
      },
      "outputs": [],
      "source": [
        "w = clasify_minst(x_train, y_train, num=800, epoch=5)\n",
        "uplot.show_n_images([w[i,0:28*28].reshape((28,28)) for i in range(10)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZiMzD4hFD3t"
      },
      "outputs": [],
      "source": [
        "w = clasify_minst(x_train, y_train, num=6000, epoch=5)\n",
        "uplot.show_n_images([w[i,0:28*28].reshape((28,28)) for i in range(10)], enlarge=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFd8W1x9FD3u"
      },
      "outputs": [],
      "source": [
        "def pred_mist(w,inputs):\n",
        "    pred=[]\n",
        "    \n",
        "    for x in inputs:\n",
        "        \n",
        "        yhat = predict(w, x)\n",
        "        pred.append(yhat)\n",
        "    return pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HYGDLD6FD3u"
      },
      "source": [
        "### Lets View some predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4k0dlDBFD3u"
      },
      "outputs": [],
      "source": [
        "p = pred_mist(w, x_train[22:30])\n",
        "print('Predictions: ',p)\n",
        "print('True lables: ', y_train[22:30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYn9s2GoFD3u"
      },
      "outputs": [],
      "source": [
        "plt.imshow(x_train[23])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGV7aAjZFD3u"
      },
      "source": [
        "#### Evaluate\n",
        "For simplicity- We will use accuracy score:<br>\n",
        "![title](d7.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXAq2DYNFD3v"
      },
      "outputs": [],
      "source": [
        "y_pred = pred_mist(w,x_train)\n",
        "print('accuracy on train group:', accuracy_score(y_train, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLsmX0T4FD3v"
      },
      "outputs": [],
      "source": [
        "y_pred_test = pred_mist(w,x_test)\n",
        "print('accuracy on test group:', accuracy_score(y_test, y_pred_test))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3MoMB8IFD3v"
      },
      "outputs": [],
      "source": [
        "from keras.utils import np_utils\n",
        "print(log_loss(np_utils.to_categorical(y_test, 10),\n",
        "               np_utils.to_categorical(y_pred_test, 10))/10.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Gsfr4R-FD3v"
      },
      "outputs": [],
      "source": [
        "print(np_utils.to_categorical(y_pred_test[0], 10))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "V2_Deep_Learning_intro_part1.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}