{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naomifridman/Introduction_to_deep_learning/blob/master/lec3_Delta_rule_learning_mlp_backpropogation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wke-dcbWGHim"
      },
      "source": [
        "# How Learning works and Multi Layer Neural Networks\n",
        "### Delta rule for learning, Gradient Decent and backpropogation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/naomifridman/Introduction_to_deep_learning.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lO4v63VjFqE",
        "outputId": "b74d83aa-9c5d-43ed-bfab-4c323a44a2c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Introduction_to_deep_learning' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Pvyij1AoGHis"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "np.set_printoptions(precision=3)\n",
        "import Introduction_to_deep_learning.utils_plot as uplot\n",
        "import matplotlib.gridspec as gridspec\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "%matplotlib inline  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwByQ7_7GHiu"
      },
      "source": [
        "Networks without hidden units are very limited in what they can learn to model. What we need is multiple layers of non-linear hidden units. <br>\n",
        "The challenge is: **how to train such networks ?**<br>\n",
        "We need a way to update all the weights not just the last layer like in a perceptron. It is a hard problem, and it took the researchers 20 years to find the right method. <br>\n",
        "\n",
        "\n",
        "![title](https://raw.githubusercontent.com/naomifridman/Introduction_to_deep_learning/master/imgs/training_model.png)\n",
        "Any hidden unit, can affect many other units, and affect the results, in many ways. So we use a method to combine all those effects, and focus on investigating the effect, the weight have, on the overall error.<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQjWxhCRGHiv"
      },
      "source": [
        "The aim of the learning is to minimize the error summed (or mean) over all training cases. <br>\n",
        "To achieve that,we need a **measure** of that error. For simplicity, we will use the square difference between the target output and the actual output. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ-HotYvGHiv"
      },
      "source": [
        "### Loss Function For learning\n",
        "The squared loss is a popular loss function, also known as L2 loss.The squared loss is the squared distance between the prediction and the true label. <br>\n",
        "Lets mark:\n",
        "* $x\\, -\\, The\\, input$\n",
        "* $\\hat{y}\\, or \\,y'\\, - \\,The\\, true\\, labels$<br>\n",
        "* $y = predictions(x) = f(x)\\; Model's\\,  predictions$<br>\n",
        "\n",
        "The squared loss for a single example is: ${(\\hat{y} - y)}^2$<br>\n",
        "\n",
        " \n",
        "#### Mean square error (MSE) \n",
        "is the average squared loss per example over the whole dataset. \n",
        "To calculate MSE, sum up all the squared\n",
        "losses for individual examples and then divide by the number of examples:\n",
        "    $$ MSE = \\frac{1}{N} \\sum_{(x,y)\\in D} (\\hat{y} - f(x))^2 = \\frac{1}{N} \\sum_{(x,y)\\in D} (\\hat{y} - y)^2$$\n",
        "> MSE is popular in machine learning, mainly from historical reasons, since all the math was initially calculated with MSE. But it is not necessary the best. For different problems, there are other practical loss functions that performs better.\n",
        "\n",
        "#### The goal of training a model is to find a set of weights that produce a minimal loss, on average, across all examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVCSQZLqGHiw"
      },
      "source": [
        "To find the minimum of the loss function, we will use An iterative method. <br>\n",
        "Iterative methods, are usually less efficient, but much easier to generalize."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WKQdnIaGHix"
      },
      "source": [
        "## Gradient Decent - Minimize loss function iterativly \n",
        "Intuition: Imagine you stand in a canyon, or a bowl, and you want to get to the lowest point. If you walk in small steps towards the steepest part, this will get you to the lowest point, if the canyon (or the bowl) is convex.\n",
        "![title](https://raw.githubusercontent.com/naomifridman/Introduction_to_deep_learning/master/imgs/dune11.jpg)\n",
        "### Delta rule for learning\n",
        "This intuition is defined as the delta rule.\n",
        "$$ \\Delta{W} = \\alpha*(derivative\\, of\\, the \\,loss\\, function)$$\n",
        "\n",
        "* $\\alpha$ - learning rate, is the (small) size of the step.\n",
        "\n",
        "The weights update will be:\n",
        "$$ W_{new} = W_{old} - \\Delta{W}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJlqL4-mGHiy"
      },
      "source": [
        "But how do we implement the delta rule for a multi layer network ?\n",
        "This implementation is called back-propagation. Back-propagation, is an abbreviation for **\"backwards propagation of errors\"**, is a mechanism used to update the weights using gradient descent. It calculates the gradient of the error function with respect to the neural networks weights. The calculation proceeds backwards through the network.\n",
        "![title](gradient_desent.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huld3tgfGHiz"
      },
      "source": [
        "## Back-propagation - Example with simple network\n",
        "Back propagation,is a method to propagate the error, back to the weights, and update them in away that will minimize the error.<br>\n",
        "Math is simple algebra, but there are some indexing to follow. The coded is illustrative, specific and not too general. Still you can play with it and test different hyper parameters and data sets.<br>\n",
        "### Multiple Layer Neural Networks example\n",
        "To implement full back-propagation example, Lets build a simple neural network, with:\n",
        "* One hidden layer, with 2 neurons\n",
        "* Input layer of 2 feature and bias\n",
        "* Output layer, outputs probability of binary classification (the probability to be 1)\n",
        "\n",
        "![title](https://raw.githubusercontent.com/naomifridman/Introduction_to_deep_learning/master/imgs/mlp1j11.JPG)\n",
        "#### Activation function\n",
        "For simplicity, we choose all activation functions to be Sigmoid function.\n",
        "\n",
        "* $z$ - will be the weighted input sum of the input to the neuron\n",
        "* $y$ - will be the output of the neuron, which is the Sigmoid(z).\n",
        "* I - the input to the neuron\n",
        "* W - the neurons weights (same number as inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eybbite1GHi0"
      },
      "source": [
        "### Sigmoid activation function\n",
        "logistic function, which ranges from 0 to 1\n",
        "Used as last layer, garanty to output propability<br>\n",
        "The logistic sigmoid function can cause a neural network to get stuck at the training time.\n",
        "#### Logistic Sigmoid:  \n",
        "## $\\sigma(x)=\\frac{1}{1+e^{-x}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "gwjcYm0lGHi0",
        "outputId": "ca5d5b6f-88e5-477e-8739-bfbca75a6c9b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 480x160 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import Introduction_to_deep_learning.utils_plot as uplot\n",
        "uplot.drow_sigmoid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsCIeNExGHi1"
      },
      "source": [
        "##### The Neuron \n",
        "All Neurons in our network, calculate weighted sum of the input, and output the results of applying Sigmoid function on the weighted sum.\n",
        "![title](https://raw.githubusercontent.com/naomifridman/Introduction_to_deep_learning/master/imgs/neuron_in_out.JPG)\n",
        "### Build MLP Step 1. Initialize network with random weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zRtndNeGHi2"
      },
      "source": [
        "### Step 2. Forward pass\n",
        "Given an input, calculate the output. To do that, we calculate all the values in the neurons along the network, until the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cY-SnvVuGHi2"
      },
      "source": [
        "## Step 3. Back-propagation\n",
        "To apply the delta rule for learning, we need to define a loss function. Lets use the simple known square error, for **one input sample**:\n",
        "$$Error = \\frac{1}{2}(\\hat{y} - y)^2$$\n",
        "where:\n",
        "* $\\hat{y}$ is the true label, y_train\n",
        "* $y$ is the prediction calculated by the network\n",
        "\n",
        "To update weights, according to delta rule, we need to calculate the \"influence\" of each weight on the target. Mathematically, we need to calculate the partial derivative of the error, with respect to each weight. <br>\n",
        "Lets mark the weights in our network, and lets mark :\n",
        "* $o$-output layer, and $h$-hidden layer\n",
        "* $y_{o}$, and $y_{h}$ are the outputs from output and hidden layers\n",
        "* $z_{o}$, and $z_{h}$ are the weighted sum of input and neuron weights, of the output and hidden layers.\n",
        "\n",
        "![title](https://raw.githubusercontent.com/naomifridman/Introduction_to_deep_learning/master/imgs/mlp_2hidden_1ou_weightst.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9saUtxfcGHi2"
      },
      "source": [
        "### Back-propagating error for output layer\n",
        "Consider $w_1$, We want to know how much a change in $w_1$ affects the total error, aka, $\\frac{\\partial Error}{\\partial w_{1}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz-muACpGHi2"
      },
      "source": [
        "Using chain rule, we will get:\n",
        "$$\\frac{\\partial Error}{\\partial w_{1}} = \\frac{\\partial Error}{\\partial y_o} * \n",
        "\\frac{\\partial y_o}{\\partial z_{o}} *\n",
        "\\frac{\\partial z_{o}}{\\partial w_{1}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtryXdITGHi3"
      },
      "source": [
        "$$\\frac{\\partial Error}{\\partial y_o} = 2 * \\frac{1}{2}(\\hat{y} - y_o)^{2 - 1} * -1 =(y_o-\\hat{y})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APh86i67GHi3"
      },
      "source": [
        "We chose Sigmoid activation, which derivative is:\n",
        "$$\\frac{\\partial}{\\partial x}Sigmoid(x) = Sigmoid(x)(1 - Sigmoid(x))$$\n",
        "\n",
        "\n",
        "And since:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHmWJCqtGHi3"
      },
      "source": [
        "\n",
        "\n",
        "$$\\hat{y} = y_o = \\frac{1}{1+e^{-z_{o}}}$$\n",
        "We get:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAxLKN83GHi3"
      },
      "source": [
        "$$\\frac{\\partial y_o}{\\partial z_{o}} = y_o*(1-y_o)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW_m1jMsGHi3"
      },
      "source": [
        "$$z_{o} = y_{h0}*w_0 + y_{h1}*w_1 = \\bar{w_{o}} * \\bar{y_h}$$    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkuRppjzGHi4"
      },
      "source": [
        "$y_{h0}$ ho has no dependence in $w_{1}$, So:\n",
        "$$\\frac{\\partial z_{o}}{\\partial w_{1}} = y_{h1}$$    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoCcBia6GHi4"
      },
      "source": [
        "\n",
        "Putting it all together, we get:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3gNhn8FGHi4"
      },
      "source": [
        "$$\\frac{\\partial Error}{\\partial w_{1}} = \\frac{\\partial Error}{\\partial y_o} * \n",
        "\\frac{\\partial y_o}{\\partial z_{o}} *\n",
        "\\frac{\\partial z_{o}}{\\partial w_{1}} =  -(\\hat{y}-y_o)*\n",
        "y_o*(1-y_o)*\n",
        "y_{h1}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEGr3gcyGHi4"
      },
      "source": [
        "So according to delta rule, to decrease error, we update $w_1$:\n",
        "$$w_1^{new} = w_1 - \\alpha*\\frac{\\partial Error}{\\partial w_{1}} = w_1 + \\alpha* (\\hat{y}-y_o)*  y_o*(1-y_o)*y_{h1}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tzwFZjKGHi4"
      },
      "source": [
        "In the same, we can calculate update for $w_0$:\n",
        "$$w_0^{new} = w_0 + \\alpha* (\\hat{y}-y_o)*  y_o*(1-y_o)*y_{ho}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPy8AqBfGHi4"
      },
      "source": [
        "Lets mark:\n",
        "$$ \\delta_{i} = y_i*(1-y_i)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M50OKHhtGHi5"
      },
      "source": [
        "**We get an update rule for output layer weights:**\n",
        "### $$w_0^{new} = w_0 + \\alpha* \\delta_{o}*y_{ho}*(\\hat{y}-y_o)$$\n",
        "### $$w_1^{new} = w_1 + \\alpha* \\delta_{o}*y_{h1}*(\\hat{y}-y_o)$$\n",
        "**Vector wise:**\n",
        "### $$\\bar{w}^{new} = \\bar{w} + \\alpha* \\delta_{o}*\\bar{y}_{h}*(\\hat{y}-y_o)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OTSs9AjGHi5"
      },
      "source": [
        "### Back-propagating error for hidden layer\n",
        "Now we need to continue one more layer back,to calculate the update of the weights.<br>\n",
        "\n",
        "In a similar way, lets calculate: $\\frac{\\partial Error}{\\partial w_{12}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VX4ABQ_GHi5"
      },
      "source": [
        "$$\\frac{\\partial Error}{\\partial w_{12}} = [\\frac{\\partial Error}{\\partial y_{o}} * \n",
        "\\frac{\\partial y_{o}}{\\partial z_{o}}] * \n",
        "\\frac{\\partial z_{o}}{\\partial y_{h1}} * \n",
        "\\frac{\\partial y_{h1}}{\\partial z_{h1}} *\n",
        "\\frac{\\partial z_{h1}}{\\partial w_{12}} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tsi5wty0GHi5"
      },
      "source": [
        "$$\\frac{\\partial Error}{\\partial w_{12}} = -[\\delta_{0}*y_{h1}*(\\hat{y}-y_0)]*\\frac{\\partial z_{o}}{\\partial y_{h1}} * \n",
        "\\frac{\\partial y_{h1}}{\\partial z_{h1}} *\n",
        "\\frac{\\partial z_{h1}}{\\partial w_{12}} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJEDwhclGHi5"
      },
      "source": [
        "$$z_{o} = y_{h_0}*w_0 + y_{h_1}*w_1 = \\bar{z_h}*\\bar{w_o}$$   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-vlogopGHi5"
      },
      "source": [
        "$$\\frac{\\partial z_{o}}{\\partial y_{h1}} = w_1$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sweKkOwjGHi5"
      },
      "source": [
        "$$\\frac{\\partial y_{h1}}{\\partial z_{h1}} = y_{h1}*(1-y_{h1})=\\delta_{h1}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tjCyX3bGHi5"
      },
      "source": [
        "$$\\frac{\\partial z_{h1}}{\\partial w_{12}} = I_{2}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op9eABrXGHi6"
      },
      "source": [
        "Putting it all together, we get:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4hCaUxVGHi6"
      },
      "source": [
        "$$\\frac{\\partial Error}{\\partial w_{10}} = -(\\hat{y}-y_o)*\\delta_{o}*y_{h1}*w_1*\\delta_{h1}*I_0$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNSpj7KXGHi6"
      },
      "source": [
        "$$\\frac{\\partial Error}{\\partial w_{11}} = -(\\hat{y}-y_o)*\\delta_{o}*y_{h1}*w_1*\\delta_{h1}*I_1$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aeha-WdcGHi6"
      },
      "source": [
        "$$\\frac{\\partial Error}{\\partial w_{12}} = -(\\hat{y}-y_o)*\\delta_{o}*y_{h1}*w_1*\\delta_{h1}*I_2$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzOwNNv7GHi6"
      },
      "source": [
        "We can write, general update rule for hidden layer of our network:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_lHTpDSGHi6"
      },
      "source": [
        "$$\\frac{\\partial Error}{\\partial w_{ij}} = -(\\hat{y}-y_o)*\\delta_{o}*y_{hi}*w_i*\\delta_{hi}*I_j$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53H61pRIGHi7"
      },
      "source": [
        "**So, update rule for hidden layer weights, will be:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YdEce1IGHi7"
      },
      "source": [
        "### $$w^{new}_{ij} = w_{ij} + \\alpha *(\\hat{y}-y_o)*\\delta_{o}*y_{hi}*w_i*\\delta_{hi}*I_j$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-eAXRoyGHi7"
      },
      "source": [
        "* details https://beckernick.github.io/sigmoid-derivative-neural-network/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lzLQQwrGHi8"
      },
      "source": [
        "## Step 4. Train network\n",
        "In the train function, in each epoch, we go over all input samples, perform forward pass on each sample, and update weights with back-propagating the error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ElDXx6uGHi8"
      },
      "source": [
        "## Step 5. Predict with the Trained Network\n",
        "To predict, with our trained network, all we need to do is to propagate forward the input. The result, is the probability that the class to which the input belong is 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyF-Oa6rGHi8"
      },
      "source": [
        "### Learn more:\n",
        "* This part was written as an attempt to understand Hinton's lecture: https://www.coursera.org/learn/neural-networks/lecture/bD3OB/learning-the-weights-of-a-linear-neuron-12-min\n",
        "* Google machine learning course https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent\n",
        "* https://en.wikipedia.org/wiki/Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKGpVTZJGHi8"
      },
      "source": [
        "### Articles:\n",
        "* D.E. RummelhartG.E. HintonR.J. Williams. **Learning Internal Representations by Error Propagation** Jan 1986 Nature."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "lec3_Delta_rule_learning_mlp_backpropogation.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}